{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a77d0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'법령별 csv 저장 중복제거' 폴더의 CSV 파일들을 읽어 '법령_통합텍스트(현역)' 폴더에 TXT 파일로 통합합니다.\n",
      "통합 대상 컬럼: ['조문내용', '항', '호', '목']\n",
      "오류: 원본 폴더 '법령별 csv 저장 중복제거'를 찾을 수 없습니다. 경로를 확인해주세요.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 원본 CSV 파일들이 있는 소스 디렉토리\n",
    "source_dir = '법령별 csv 저장 중복제거'\n",
    "\n",
    "# 생성된 .txt 파일들을 저장할 출력 디렉토리\n",
    "output_dir = '법령_통합텍스트(현역)'\n",
    "\n",
    "# 합칠 대상 컬럼 목록\n",
    "# ★ 만약 다른 컬럼도 추가하고 싶다면 이 리스트에 추가하세요.\n",
    "columns_to_combine = ['조문내용', '항', '호', '목']\n",
    "\n",
    "# 출력 디렉토리 생성 (이미 존재하면 넘어감)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"'{source_dir}' 폴더의 CSV 파일들을 읽어 '{output_dir}' 폴더에 TXT 파일로 통합합니다.\")\n",
    "print(f\"통합 대상 컬럼: {columns_to_combine}\")\n",
    "\n",
    "# 소스 디렉토리의 모든 파일에 대해 반복 작업\n",
    "if not os.path.isdir(source_dir):\n",
    "    print(f\"오류: 원본 폴더 '{source_dir}'를 찾을 수 없습니다. 경로를 확인해주세요.\")\n",
    "else:\n",
    "    for filename in tqdm(os.listdir(source_dir)):\n",
    "        if filename.endswith('.csv'):\n",
    "            try:\n",
    "                source_path = os.path.join(source_dir, filename)\n",
    "                df = pd.read_csv(source_path, sep='\\t')\n",
    "\n",
    "                # 실제 파일에 존재하는 컬럼만 필터링\n",
    "                existing_columns = [col for col in columns_to_combine if col in df.columns]\n",
    "\n",
    "                # 해당 컬럼들의 빈 값(NaN)을 빈 문자열('')로 대체\n",
    "                df[existing_columns] = df[existing_columns].fillna('').astype(str)\n",
    "\n",
    "                # 각 행(row)별로 존재하는 컬럼 내용을 공백으로 이어붙임\n",
    "                combined_series = df[existing_columns].apply(lambda row: ' '.join(row.values), axis=1)\n",
    "\n",
    "                # 법령 전체의 텍스트를 하나의 문자열로 합침 (각 행은 줄바꿈으로 구분)\n",
    "                full_text = '\\n'.join(combined_series)\n",
    "\n",
    "                # 출력 파일 경로 설정 (예: '가사소송법.csv' -> '가사소송법.txt')\n",
    "                output_filename = filename.replace('.csv', '.txt')\n",
    "                output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "                # 통합된 텍스트를 .txt 파일로 저장\n",
    "                with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(full_text)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"오류 발생: {filename} 처리 중 문제 발생 - {e}\")\n",
    "\n",
    "    print(f\"\\n작업 완료! 모든 법령이 '{output_dir}' 폴더에 .txt 파일로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d50434a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Javier\\anaconda3\\envs\\legal_text\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okt 토크나이저를 사용합니다.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1. 라이브러리 및 환경 설정\n",
    "# ==============================================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # 진행 상황을 시각적으로 보여주는 라이브러리\n",
    "\n",
    "# --- 텍스트 처리를 위한 라이브러리 ---\n",
    "from konlpy.tag import Okt # Okt이 설치되지 않았다면 Okt로 변경: from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# --- 딥러닝 및 유사도 계산을 위한 라이브러리 ---\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "\n",
    "# tqdm을 pandas의 apply와 함께 사용하기 위해 필요한 설정\n",
    "tqdm.pandas()\n",
    "\n",
    "# Okt 초기화 (Okt 설치 경로에 따라 필요시 인자 추가)\n",
    "# okt를 사용할 경우: tokenizer = Okt()\n",
    "try:\n",
    "    tokenizer = Okt()\n",
    "    print(\"Okt 토크나이저를 사용합니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"Okt 로드 실패: {e}\")\n",
    "    print(\"Okt 토크나이저로 대체합니다. 처리 속도가 느릴 수 있습니다.\")\n",
    "    from konlpy.tag import Okt\n",
    "    tokenizer = Okt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ce56e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "과목별 법령 목록 로드 완료.\n",
      "'법령_통합텍스트(현역)' 폴더에서 전체 법령 텍스트를 로드합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5502/5502 [01:27<00:00, 62.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 법령 데이터 로드 완료: 총 5502개 법령\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from konlpy.tag import Mecab\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# (이하 필요한 라이브러리들)\n",
    "\n",
    "try:\n",
    "    # ★ 탭으로 분리된 과목 목록 파일을 읽도록 sep='\\t' 추가\n",
    "    public_laws = pd.read_csv('변호사시험_출제대상_부속법령(공법).csv')['법령명'].tolist()\n",
    "    civil_laws = pd.read_csv('변호사시험_출제대상_부속법령(민사법).csv')['법령명'].tolist()\n",
    "    criminal_laws = pd.read_csv('변호사시험_출제대상_부속법령(형사법).csv')['법령명'].tolist()\n",
    "    print(\"과목별 법령 목록 로드 완료.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"오류: 과목별 법령 목록 파일을 찾을 수 없습니다. ({e.filename})\")\n",
    "    exit() # 스크립트에서는 주석 해제\n",
    "\n",
    "# --- 전체 법령 데이터 로드 (생성된 .txt 파일 로드) ---\n",
    "all_laws_dir = '법령_통합텍스트(현역)' # ★ .txt 파일들이 저장된 폴더\n",
    "all_laws_data = []\n",
    "\n",
    "\n",
    "print(f\"'{all_laws_dir}' 폴더에서 전체 법령 텍스트를 로드합니다...\")\n",
    "if not os.path.isdir(all_laws_dir):\n",
    "    print(f\"오류: '{all_laws_dir}' 폴더를 찾을 수 없습니다. 1단계 코드를 먼저 실행했는지 확인해주세요.\")\n",
    "    exit()\n",
    "\n",
    "for file_name in tqdm(os.listdir(all_laws_dir)):\n",
    "    if file_name.endswith('.txt'):\n",
    "        law_name = file_name[:-4]  # '.txt' 확장자 제거\n",
    "        file_path = os.path.join(all_laws_dir, file_name)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            all_laws_data.append({'법령명': law_name, '내용': content})\n",
    "        except Exception as e:\n",
    "            print(f\"에러: {file_name} 파일 처리 중 오류 발생 - {e}\")\n",
    "\n",
    "df_all_laws = pd.DataFrame(all_laws_data)\n",
    "print(f\"전체 법령 데이터 로드 완료: 총 {len(df_all_laws)}개 법령\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "276eb56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. 데이터 전처리 시작 (명사 추출) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 4458/5502 [30:32<07:09,  2.43it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# '내용' 컬럼에 전처리 함수 적용. 시간이 다소 소요될 수 있습니다.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m tqdm\u001b[38;5;241m.\u001b[39mpandas()\n\u001b[1;32m---> 22\u001b[0m df_all_laws[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m내용_전처리\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_all_laws\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m내용\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m데이터 전처리 완료.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_all_laws[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m법령명\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m내용_전처리\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32mc:\\Users\\Javier\\anaconda3\\envs\\legal_text\\lib\\site-packages\\tqdm\\std.py:917\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[1;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(df, df_function)(wrapper, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Javier\\anaconda3\\envs\\legal_text\\lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Javier\\anaconda3\\envs\\legal_text\\lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Javier\\anaconda3\\envs\\legal_text\\lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\Javier\\anaconda3\\envs\\legal_text\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Javier\\anaconda3\\envs\\legal_text\\lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Javier\\anaconda3\\envs\\legal_text\\lib\\site-packages\\tqdm\\std.py:912\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[0;32m    908\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[0;32m    911\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Mecab을 이용해 텍스트에서 명사만 추출하고 한 글자 단어는 제거\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Mecab 사용 시: nouns = tokenizer.nouns(text)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Okt 사용 시: nouns = tokenizer.nouns(text)\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     nouns \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnouns\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# 한 글자 명사 제거 후 공백으로 연결\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([noun \u001b[38;5;28;01mfor\u001b[39;00m noun \u001b[38;5;129;01min\u001b[39;00m nouns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(noun) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Javier\\anaconda3\\envs\\legal_text\\lib\\site-packages\\konlpy\\tag\\_okt.py:83\u001b[0m, in \u001b[0;36mOkt.nouns\u001b[1;34m(self, phrase)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnouns\u001b[39m(\u001b[38;5;28mself\u001b[39m, phrase):\n\u001b[0;32m     81\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Noun extractor.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m     tagged \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [s \u001b[38;5;28;01mfor\u001b[39;00m s, t \u001b[38;5;129;01min\u001b[39;00m tagged \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNoun\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Javier\\anaconda3\\envs\\legal_text\\lib\\site-packages\\konlpy\\tag\\_okt.py:71\u001b[0m, in \u001b[0;36mOkt.pos\u001b[1;34m(self, phrase, norm, stem, join)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"POS tagger.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03mIn contrast to other classes in this subpackage,\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03mthis POS tagger doesn't have a `flatten` option,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m:param join: If True, returns joined sets of morph and tag.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     69\u001b[0m validate_phrase_inputs(phrase)\n\u001b[1;32m---> 71\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjki\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjpype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBoolean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjpype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBoolean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoArray()\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m join:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 3. 데이터 전처리 (형태소 분석 및 명사 추출)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 3. 데이터 전처리 시작 (명사 추출) ---\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Mecab을 이용해 텍스트에서 명사만 추출하고 한 글자 단어는 제거\"\"\"\n",
    "    try:\n",
    "        # Mecab 사용 시: nouns = tokenizer.nouns(text)\n",
    "        # Okt 사용 시: nouns = tokenizer.nouns(text)\n",
    "        nouns = tokenizer.nouns(str(text))\n",
    "        # 한 글자 명사 제거 후 공백으로 연결\n",
    "        return ' '.join([noun for noun in nouns if len(noun) > 1])\n",
    "    except Exception as e:\n",
    "        # print(f\"전처리 오류 발생: {e}, 원문: {text[:30]}\")\n",
    "        return \"\" # 오류 발생 시 빈 문자열 반환\n",
    "\n",
    "# '내용' 컬럼에 전처리 함수 적용. 시간이 다소 소요될 수 있습니다.\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "df_all_laws['내용_전처리'] = df_all_laws['내용'].progress_apply(preprocess_text)\n",
    "print(\"데이터 전처리 완료.\")\n",
    "print(df_all_laws[['법령명', '내용_전처리']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b9b651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "              🔍 전처리 과정 진단 시작\n",
      "==================================================\n",
      "\n",
      "[1단계] 원본 텍스트 샘플 (앞 200자):\n",
      "제1조(목적) 이 규칙은 「공증인법」 제13조의2, 제15조의6, 제24조, 제75조 및 제77조의9제1항에 따른 공증사무소의 시설 기준, 서류의 보존 방법, 보존 장소, 보존 기간, 폐기, 서류인계의 기준, 서류 통합보관시설의 기준 및 허가 절차, 그 밖의 설비에 관하여 필요한 사항을 규정함을 목적으로 한다.\n",
      "   \n",
      "제2조(정의) 이 규칙에서 서류란 「공\n",
      "\n",
      "[2단계] Okt 형태소 분석기를 사용합니다.\n",
      "\n",
      "[3단계] 형태소 분석기가 추출한 '명사' 목록 (앞 30개):\n",
      "['제', '목적', '이', '규칙', '공증인', '법', '제', '제', '제', '제', '및', '제', '제', '항', '공증', '사무소', '시설', '기준', '서류', '보존', '방법', '보존', '장소', '보존', '기간', '폐기', '서류', '인계', '기준', '서류']\n",
      "\n",
      "[4단계] 한 글자를 제외하고 남은 '명사' 목록 (앞 30개):\n",
      "['목적', '규칙', '공증인', '공증', '사무소', '시설', '기준', '서류', '보존', '방법', '보존', '장소', '보존', '기간', '폐기', '서류', '인계', '기준', '서류', '통합', '보관', '시설', '기준', '허가', '절차', '설비', '사항', '규정', '목적', '정의']\n",
      "\n",
      "[5단계] 최종 전처리 결과 (앞 200자):\n",
      "목적 규칙 공증인 공증 사무소 시설 기준 서류 보존 방법 보존 장소 보존 기간 폐기 서류 인계 기준 서류 통합 보관 시설 기준 허가 절차 설비 사항 규정 목적 정의 규칙 서류 공증인 이하 서류 마이크로필름 전산 정보처리 조직 이하 전산 정보처리 조직 보존 서류 서류 보존 방법 증서 원본 사서 증서 인증서 사본 정관 법인 의사록 표지 붙이 증서 번호 등부 번\n"
     ]
    }
   ],
   "source": [
    "# 2. 비어있지 않은 법령 텍스트 하나를 샘플로 선택\n",
    "sample_text = \"\"\n",
    "for text in df_all_laws['내용']:\n",
    "    if text and text.strip():\n",
    "        sample_text = text\n",
    "        break\n",
    "\n",
    "if not sample_text:\n",
    "    print(\"오류: 데이터프레임의 '내용' 컬럼이 모두 비어있습니다. 파일 로딩 과정을 다시 확인해주세요.\")\n",
    "else:\n",
    "    print(\"=\"*50)\n",
    "    print(\"              🔍 전처리 과정 진단 시작\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # --- 진단 시작 ---\n",
    "    print(\"\\n[1단계] 원본 텍스트 샘플 (앞 200자):\")\n",
    "    print(sample_text[:200])\n",
    "\n",
    "    # 형태소 분석기 초기화\n",
    "    try:\n",
    "        tokenizer = Mecab()\n",
    "        print(\"\\n[2단계] Mecab 형태소 분석기를 사용합니다.\")\n",
    "    except:\n",
    "        from konlpy.tag import Okt\n",
    "        tokenizer = Okt()\n",
    "        print(\"\\n[2단계] Okt 형태소 분석기를 사용합니다.\")\n",
    "\n",
    "    # 명사 추출 실행\n",
    "    nouns_list = tokenizer.nouns(sample_text)\n",
    "    print(\"\\n[3단계] 형태소 분석기가 추출한 '명사' 목록 (앞 30개):\")\n",
    "    print(nouns_list[:30])\n",
    "\n",
    "    # 한 글자 명사 제거\n",
    "    filtered_nouns = [noun for noun in nouns_list if len(noun) > 1]\n",
    "    print(\"\\n[4단계] 한 글자를 제외하고 남은 '명사' 목록 (앞 30개):\")\n",
    "    print(filtered_nouns[:30])\n",
    "\n",
    "    # 최종 결과\n",
    "    final_result = \" \".join(filtered_nouns)\n",
    "    print(\"\\n[5단계] 최종 전처리 결과 (앞 200자):\")\n",
    "    print(final_result[:200])\n",
    "    \n",
    "    if not final_result:\n",
    "        print(\"\\n🚨 진단 결과: 최종 결과가 비어있습니다.\")\n",
    "        if not nouns_list:\n",
    "            print(\"-> 원인: 3단계에서 형태소 분석기가 명사를 전혀 추출하지 못했습니다. KoNLPy나 Mecab 설치가 올바른지 확인해주세요.\")\n",
    "        elif not filtered_nouns:\n",
    "            print(\"-> 원인: 4단계에서 모든 명사가 한 글자라 제거되었습니다. 법령 텍스트 특성일 수 있으나, 3단계 결과가 이상하다면 토크나이저 문제를 의심해봐야 합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39e532c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 결과가 'preprocessed_laws.csv' 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# # --- ★★★ 전처리 결과 저장 코드 (이 부분을 추가하세요) ★★★ ---\n",
    "# # 필요한 컬럼만 선택\n",
    "# df_to_save = df_all_laws[['법령명', '내용', '내용_전처리']]\n",
    "\n",
    "# # CSV 파일로 저장 (index=False는 불필요한 인덱스 컬럼이 생기는 것을 방지)\n",
    "# output_filename = 'preprocessed_laws.csv'\n",
    "# df_to_save.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# print(f\"전처리 결과가 '{output_filename}' 파일로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfec8cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "전처리된 텍스트를 '법령_전처리_텍스트' 폴더에 개별 저장합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5502/5502 [00:11<00:00, 466.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "작업 완료! 모든 전처리된 법령이 '법령_전처리_텍스트' 폴더에 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # 전처리된 파일들을 저장할 새 폴더 이름\n",
    "# preprocessed_output_dir = '법령_전처리_텍스트' \n",
    "\n",
    "# # 새 폴더 생성 (이미 있으면 넘어감)\n",
    "# os.makedirs(preprocessed_output_dir, exist_ok=True)\n",
    "# print(f\"\\n전처리된 텍스트를 '{preprocessed_output_dir}' 폴더에 개별 저장합니다...\")\n",
    "\n",
    "# # 데이터프레임을 한 줄씩 순회하며 파일로 저장\n",
    "# # df_all_laws는 전처리가 완료된 데이터프레임입니다.\n",
    "# for index, row in tqdm(df_all_laws.iterrows(), total=df_all_laws.shape[0]):\n",
    "#     law_name = row['법령명']\n",
    "#     preprocessed_content = row['내용_전처리']\n",
    "    \n",
    "#     # 출력 파일 경로 설정\n",
    "#     output_path = os.path.join(preprocessed_output_dir, f\"{law_name}.txt\")\n",
    "    \n",
    "#     # 전처리된 내용을 .txt 파일로 저장\n",
    "#     with open(output_path, 'w', encoding='utf-8') as f:\n",
    "#         f.write(preprocessed_content)\n",
    "\n",
    "# print(f\"\\n작업 완료! 모든 전처리된 법령이 '{preprocessed_output_dir}' 폴더에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c1d9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 데이터 전처리 시작 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5502/5502 [31:23<00:00,  2.92it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> 데이터 전처리 완료.\n",
      "\n",
      "-> 전처리된 텍스트를 '법령_전처리_텍스트' 폴더에 개별 저장합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5502/5502 [00:15<00:00, 363.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 저장 성공! '법령_전처리_텍스트' 폴더를 확인해보세요.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 3. 데이터 전처리 ---\n",
    "print(\"\\n--- 데이터 전처리 시작 ---\")\n",
    "df_all_laws['내용_전처리'] = df_all_laws['내용'].progress_apply(preprocess_text)\n",
    "print(\"-> 데이터 전처리 완료.\")\n",
    "\n",
    "# --- 4. 전처리 결과를 개별 txt 파일로 저장 ---\n",
    "preprocessed_output_dir = '법령_전처리_텍스트'\n",
    "os.makedirs(preprocessed_output_dir, exist_ok=True)\n",
    "print(f\"\\n-> 전처리된 텍스트를 '{preprocessed_output_dir}' 폴더에 개별 저장합니다...\")\n",
    "\n",
    "try:\n",
    "    for index, row in tqdm(df_all_laws.iterrows(), total=df_all_laws.shape[0]):\n",
    "        law_name = row['법령명']\n",
    "        preprocessed_content = row['내용_전처리']\n",
    "        output_path = os.path.join(preprocessed_output_dir, f\"{law_name}.txt\")\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(preprocessed_content)\n",
    "    print(f\"\\n✅ 저장 성공! '{preprocessed_output_dir}' 폴더를 확인해보세요.\")\n",
    "except Exception as e:\n",
    "    print(f\"🚨 저장 실패: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831ad637",
   "metadata": {},
   "source": [
    "### 유사도 분석함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ab25a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_cluster_similarity(all_vectors, all_law_names, subject_law_list, subject_name):\n",
    "    \"\"\"\n",
    "    주어진 과목 클러스터의 내부 유사도와 외부 유사도를 계산하는 함수.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[{subject_name}] 클러스터 유사도 분석 중...\")\n",
    "    subject_indices = all_law_names[all_law_names.isin(subject_law_list)].index.tolist()\n",
    "    external_indices = all_law_names[~all_law_names.isin(subject_law_list)].index.tolist()\n",
    "\n",
    "    if len(subject_indices) == 0:\n",
    "        return 0, 0\n",
    "\n",
    "    subject_vectors = all_vectors[subject_indices]\n",
    "    external_vectors = all_vectors[external_indices]\n",
    "    \n",
    "    internal_sim_avg = 0\n",
    "    if subject_vectors.shape[0] > 1:\n",
    "      internal_sim_matrix = cosine_similarity(subject_vectors)\n",
    "      internal_sim_avg = np.mean(internal_sim_matrix[np.triu_indices_from(internal_sim_matrix, k=1)])\n",
    "\n",
    "    external_sim_avg = 0\n",
    "    if len(external_indices) > 0 and len(subject_indices) > 0:\n",
    "      external_sim_matrix = cosine_similarity(subject_vectors, external_vectors)\n",
    "      external_sim_avg = np.mean(external_sim_matrix)\n",
    "\n",
    "    print(f\" - 내부 유사도: {internal_sim_avg:.4f}, 외부 유사도: {external_sim_avg:.4f}\")\n",
    "    return internal_sim_avg, external_sim_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7777567",
   "metadata": {},
   "source": [
    "# TF-IDF 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1bb00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TF-IDF 분석 시작 ---\n",
      "TF-IDF 벡터화 완료. 벡터 shape: (5502, 5000)\n",
      "\n",
      "[공법 (TF-IDF)] 클러스터 유사도 분석 중...\n",
      " - 내부 유사도: 0.0946, 외부 유사도: 0.0538\n",
      "\n",
      "[민사법 (TF-IDF)] 클러스터 유사도 분석 중...\n",
      " - 내부 유사도: 0.1214, 외부 유사도: 0.0445\n",
      "\n",
      "[형사법 (TF-IDF)] 클러스터 유사도 분석 중...\n",
      " - 내부 유사도: 0.1103, 외부 유사도: 0.0520\n",
      "\n",
      "\n",
      "==============================\n",
      "     TF-IDF 분석 결과\n",
      "==============================\n",
      "       내부 유사도    외부 유사도\n",
      "과목명                    \n",
      "공법   0.094576  0.053769\n",
      "민사법  0.121437  0.044462\n",
      "형사법  0.110265  0.052035\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# --- 4-1. TF-IDF 벡터화 ---\n",
    "print(\"\\n--- TF-IDF 분석 시작 ---\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, min_df=5)\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(df_all_laws['내용_전처리'])\n",
    "print(\"TF-IDF 벡터화 완료. 벡터 shape:\", tfidf_vectors.shape)\n",
    "\n",
    "# --- 6-1. TF-IDF 유사도 분석 실행 ---\n",
    "tfidf_results = []\n",
    "subjects = {\"공법\": public_laws, \"민사법\": civil_laws, \"형사법\": criminal_laws}\n",
    "all_law_names_series = df_all_laws['법령명']\n",
    "\n",
    "for subject_name, law_list in subjects.items():\n",
    "    internal_sim, external_sim = analyze_cluster_similarity(\n",
    "        tfidf_vectors, all_law_names_series, law_list, f\"{subject_name} (TF-IDF)\"\n",
    "    )\n",
    "    tfidf_results.append({\n",
    "        \"과목명\": subject_name,\n",
    "        \"내부 유사도\": internal_sim,\n",
    "        \"외부 유사도\": external_sim\n",
    "    })\n",
    "\n",
    "# --- 7-1. TF-IDF 최종 결과 출력 ---\n",
    "print(\"\\n\\n\" + \"=\"*30)\n",
    "print(\"     TF-IDF 분석 결과\")\n",
    "print(\"=\"*30)\n",
    "df_tfidf_results = pd.DataFrame(tfidf_results).set_index('과목명')\n",
    "print(df_tfidf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbceb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf_results.to_csv(\"sbert_과목별_결과_현역.idf\", encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9320ce56",
   "metadata": {},
   "source": [
    "### SBERT 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592f25ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'법령_통합텍스트(현역)' 폴더에서 원본 법령 텍스트를 로드합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5502/5502 [00:00<00:00, 5582.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "원본 데이터 로딩 완료: 총 5502개 법령\n",
      "과목별 법령 목록 로드 완료.\n",
      "\n",
      "\n",
      "--- SBERT 분석 시작 ---\n",
      "SBERT 분석에 사용할 장치: CUDA\n",
      "SBERT 모델을 로드합니다. 최초 실행 시 수 분이 소요될 수 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\silve\\anaconda3\\envs\\nlp\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\silve\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-multilingual-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT 인코딩을 시작합니다. 법령 개수에 따라 많은 시간이 소요될 수 있습니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 172/172 [00:23<00:00,  7.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT 벡터화 완료. 벡터 shape: (5502, 768)\n",
      "\n",
      "[공법 (SBERT)] 클러스터 유사도 분석 중...\n",
      " - 내부 유사도: 0.7114, 외부 유사도: 0.6637\n",
      "\n",
      "[민사법 (SBERT)] 클러스터 유사도 분석 중...\n",
      " - 내부 유사도: 0.7786, 외부 유사도: 0.6843\n",
      "\n",
      "[형사법 (SBERT)] 클러스터 유사도 분석 중...\n",
      " - 내부 유사도: 0.6869, 외부 유사도: 0.6564\n",
      "\n",
      "\n",
      "==============================\n",
      "      SBERT 분석 결과\n",
      "==============================\n",
      "       내부 유사도    외부 유사도\n",
      "과목명                    \n",
      "공법   0.711350  0.663651\n",
      "민사법  0.778606  0.684268\n",
      "형사법  0.686883  0.656386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# --- 1. 원본 텍스트 데이터 로드 ---\n",
    "# SBERT는 원본 텍스트를 사용하므로, 원본 .txt 파일이 있는 폴더를 지정합니다.\n",
    "original_text_dir = '법령_통합텍스트(현역)' \n",
    "all_laws_data = []\n",
    "\n",
    "print(f\"'{original_text_dir}' 폴더에서 원본 법령 텍스트를 로드합니다...\")\n",
    "if not os.path.isdir(original_text_dir):\n",
    "    print(f\"오류: '{original_text_dir}' 폴더를 찾을 수 없습니다. 원본 txt 통합 파일이 준비되어 있는지 확인해주세요.\")\n",
    "    exit()\n",
    "\n",
    "for filename in tqdm(os.listdir(original_text_dir)):\n",
    "    if filename.endswith('.txt'):\n",
    "        law_name = filename[:-4]\n",
    "        file_path = os.path.join(original_text_dir, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        all_laws_data.append({\n",
    "            '법령명': law_name,\n",
    "            '내용': content  # SBERT는 전처리 전 '내용' 컬럼을 사용\n",
    "        })\n",
    "\n",
    "df_all_laws = pd.DataFrame(all_laws_data)\n",
    "print(f\"\\n원본 데이터 로딩 완료: 총 {len(df_all_laws)}개 법령\")\n",
    "\n",
    "\n",
    "# --- 2. 과목별 법령 목록 로드 ---\n",
    "try:\n",
    "    public_laws = pd.read_csv('변호사시험_출제대상_부속법령(공법).csv', sep='\\t')['법령명'].tolist()\n",
    "    civil_laws = pd.read_csv('변호사시험_출제대상_부속법령(민사법).csv', sep='\\t')['법령명'].tolist()\n",
    "    criminal_laws = pd.read_csv('변호사시험_출제대상_부속법령(형사법).csv', sep='\\t')['법령명'].tolist()\n",
    "    print(\"과목별 법령 목록 로드 완료.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"오류: 과목별 법령 목록 파일을 찾을 수 없습니다. ({e.filename})\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- 3. 유사도 분석 함수 정의 (TF-IDF 분석 때와 동일) ---\n",
    "def analyze_cluster_similarity(all_vectors, all_law_names, subject_law_list, subject_name):\n",
    "    print(f\"\\n[{subject_name}] 클러스터 유사도 분석 중...\")\n",
    "    subject_indices = all_law_names[all_law_names.isin(subject_law_list)].index.tolist()\n",
    "    external_indices = all_law_names[~all_law_names.isin(subject_law_list)].index.tolist()\n",
    "\n",
    "    if len(subject_indices) == 0:\n",
    "        return 0, 0\n",
    "\n",
    "    subject_vectors = all_vectors[subject_indices]\n",
    "    external_vectors = all_vectors[external_indices]\n",
    "\n",
    "    internal_sim_avg = 0\n",
    "    if subject_vectors.shape[0] > 1:\n",
    "        internal_sim_matrix = cosine_similarity(subject_vectors)\n",
    "        internal_sim_avg = np.mean(internal_sim_matrix[np.triu_indices_from(internal_sim_matrix, k=1)])\n",
    "\n",
    "    external_sim_avg = 0\n",
    "    if len(external_indices) > 0 and len(subject_indices) > 0:\n",
    "        external_sim_matrix = cosine_similarity(subject_vectors, external_vectors)\n",
    "        external_sim_avg = np.mean(external_sim_matrix)\n",
    "\n",
    "    print(f\" - 내부 유사도: {internal_sim_avg:.4f}, 외부 유사도: {external_sim_avg:.4f}\")\n",
    "    return internal_sim_avg, external_sim_avg\n",
    "\n",
    "\n",
    "# --- 4. SBERT 분석 실행 ---\n",
    "print(\"\\n\\n--- SBERT 분석 시작 ---\")\n",
    "\n",
    "# GPU 사용 가능 여부 확인\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"SBERT 분석에 사용할 장치: {device.upper()}\")\n",
    "\n",
    "# SBERT 모델 로드\n",
    "print(\"SBERT 모델을 로드합니다. 최초 실행 시 수 분이 소요될 수 있습니다.\")\n",
    "sbert_model = SentenceTransformer(\n",
    "    'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"SBERT 인코딩을 시작합니다. 법령 개수에 따라 많은 시간이 소요될 수 있습니다...\")\n",
    "sbert_vectors = sbert_model.encode(\n",
    "    df_all_laws['내용'].tolist(), \n",
    "    show_progress_bar=True, \n",
    "    batch_size=32 \n",
    ")\n",
    "print(\"SBERT 벡터화 완료. 벡터 shape:\", sbert_vectors.shape)\n",
    "\n",
    "\n",
    "# --- 5. SBERT 유사도 계산 및 결과 취합 ---\n",
    "sbert_results = []\n",
    "subjects = {\"공법\": public_laws, \"민사법\": civil_laws, \"형사법\": criminal_laws}\n",
    "all_law_names_series = df_all_laws['법령명']\n",
    "\n",
    "for subject_name, law_list in subjects.items():\n",
    "    internal_sim, external_sim = analyze_cluster_similarity(\n",
    "        sbert_vectors, all_law_names_series, law_list, f\"{subject_name} (SBERT)\"\n",
    "    )\n",
    "    sbert_results.append({\n",
    "        \"과목명\": subject_name,\n",
    "        \"내부 유사도\": internal_sim,\n",
    "        \"외부 유사도\": external_sim\n",
    "    })\n",
    "\n",
    "# --- 6. 최종 결과 출력 ---\n",
    "print(\"\\n\\n\" + \"=\"*30)\n",
    "print(\"      SBERT 분석 결과\")\n",
    "print(\"=\"*30)\n",
    "df_sbert_results = pd.DataFrame(sbert_results).set_index('과목명')\n",
    "print(df_sbert_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4808b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbert_results.to_csv(\"sbert_과목별_결과_현역.csv\", encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a34db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal_text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

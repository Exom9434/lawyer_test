{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a77d0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ë²•ë ¹ë³„ csv ì €ì¥ ì¤‘ë³µì œê±°' í´ë”ì˜ CSV íŒŒì¼ë“¤ì„ ì½ì–´ 'ë²•ë ¹_í†µí•©í…ìŠ¤íŠ¸(í˜„ì—­)' í´ë”ì— TXT íŒŒì¼ë¡œ í†µí•©í•©ë‹ˆë‹¤.\n",
      "í†µí•© ëŒ€ìƒ ì»¬ëŸ¼: ['ì¡°ë¬¸ë‚´ìš©', 'í•­', 'í˜¸', 'ëª©']\n",
      "ì˜¤ë¥˜: ì›ë³¸ í´ë” 'ë²•ë ¹ë³„ csv ì €ì¥ ì¤‘ë³µì œê±°'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ì›ë³¸ CSV íŒŒì¼ë“¤ì´ ìˆëŠ” ì†ŒìŠ¤ ë””ë ‰í† ë¦¬\n",
    "source_dir = 'ë²•ë ¹ë³„ csv ì €ì¥ ì¤‘ë³µì œê±°'\n",
    "\n",
    "# ìƒì„±ëœ .txt íŒŒì¼ë“¤ì„ ì €ì¥í•  ì¶œë ¥ ë””ë ‰í† ë¦¬\n",
    "output_dir = 'ë²•ë ¹_í†µí•©í…ìŠ¤íŠ¸(í˜„ì—­)'\n",
    "\n",
    "# í•©ì¹  ëŒ€ìƒ ì»¬ëŸ¼ ëª©ë¡\n",
    "# â˜… ë§Œì•½ ë‹¤ë¥¸ ì»¬ëŸ¼ë„ ì¶”ê°€í•˜ê³  ì‹¶ë‹¤ë©´ ì´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•˜ì„¸ìš”.\n",
    "columns_to_combine = ['ì¡°ë¬¸ë‚´ìš©', 'í•­', 'í˜¸', 'ëª©']\n",
    "\n",
    "# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± (ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë„˜ì–´ê°)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"'{source_dir}' í´ë”ì˜ CSV íŒŒì¼ë“¤ì„ ì½ì–´ '{output_dir}' í´ë”ì— TXT íŒŒì¼ë¡œ í†µí•©í•©ë‹ˆë‹¤.\")\n",
    "print(f\"í†µí•© ëŒ€ìƒ ì»¬ëŸ¼: {columns_to_combine}\")\n",
    "\n",
    "# ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼ì— ëŒ€í•´ ë°˜ë³µ ì‘ì—…\n",
    "if not os.path.isdir(source_dir):\n",
    "    print(f\"ì˜¤ë¥˜: ì›ë³¸ í´ë” '{source_dir}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "else:\n",
    "    for filename in tqdm(os.listdir(source_dir)):\n",
    "        if filename.endswith('.csv'):\n",
    "            try:\n",
    "                source_path = os.path.join(source_dir, filename)\n",
    "                df = pd.read_csv(source_path, sep='\\t')\n",
    "\n",
    "                # ì‹¤ì œ íŒŒì¼ì— ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§\n",
    "                existing_columns = [col for col in columns_to_combine if col in df.columns]\n",
    "\n",
    "                # í•´ë‹¹ ì»¬ëŸ¼ë“¤ì˜ ë¹ˆ ê°’(NaN)ì„ ë¹ˆ ë¬¸ìì—´('')ë¡œ ëŒ€ì²´\n",
    "                df[existing_columns] = df[existing_columns].fillna('').astype(str)\n",
    "\n",
    "                # ê° í–‰(row)ë³„ë¡œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ ë‚´ìš©ì„ ê³µë°±ìœ¼ë¡œ ì´ì–´ë¶™ì„\n",
    "                combined_series = df[existing_columns].apply(lambda row: ' '.join(row.values), axis=1)\n",
    "\n",
    "                # ë²•ë ¹ ì „ì²´ì˜ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹¨ (ê° í–‰ì€ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)\n",
    "                full_text = '\\n'.join(combined_series)\n",
    "\n",
    "                # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ì„¤ì • (ì˜ˆ: 'ê°€ì‚¬ì†Œì†¡ë²•.csv' -> 'ê°€ì‚¬ì†Œì†¡ë²•.txt')\n",
    "                output_filename = filename.replace('.csv', '.txt')\n",
    "                output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "                # í†µí•©ëœ í…ìŠ¤íŠ¸ë¥¼ .txt íŒŒì¼ë¡œ ì €ì¥\n",
    "                with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(full_text)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"ì˜¤ë¥˜ ë°œìƒ: {filename} ì²˜ë¦¬ ì¤‘ ë¬¸ì œ ë°œìƒ - {e}\")\n",
    "\n",
    "    print(f\"\\nì‘ì—… ì™„ë£Œ! ëª¨ë“  ë²•ë ¹ì´ '{output_dir}' í´ë”ì— .txt íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d50434a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Javier\\anaconda3\\envs\\legal_text\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okt í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° í™˜ê²½ ì„¤ì •\n",
    "# ==============================================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # ì§„í–‰ ìƒí™©ì„ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "\n",
    "# --- í…ìŠ¤íŠ¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---\n",
    "from konlpy.tag import Okt # Oktì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ë‹¤ë©´ Oktë¡œ ë³€ê²½: from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# --- ë”¥ëŸ¬ë‹ ë° ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "\n",
    "# tqdmì„ pandasì˜ applyì™€ í•¨ê»˜ ì‚¬ìš©í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì„¤ì •\n",
    "tqdm.pandas()\n",
    "\n",
    "# Okt ì´ˆê¸°í™” (Okt ì„¤ì¹˜ ê²½ë¡œì— ë”°ë¼ í•„ìš”ì‹œ ì¸ì ì¶”ê°€)\n",
    "# oktë¥¼ ì‚¬ìš©í•  ê²½ìš°: tokenizer = Okt()\n",
    "try:\n",
    "    tokenizer = Okt()\n",
    "    print(\"Okt í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "except Exception as e:\n",
    "    print(f\"Okt ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"Okt í† í¬ë‚˜ì´ì €ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì†ë„ê°€ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "    from konlpy.tag import Okt\n",
    "    tokenizer = Okt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ce56e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê³¼ëª©ë³„ ë²•ë ¹ ëª©ë¡ ë¡œë“œ ì™„ë£Œ.\n",
      "'ë²•ë ¹_í†µí•©í…ìŠ¤íŠ¸(í˜„ì—­)' í´ë”ì—ì„œ ì „ì²´ ë²•ë ¹ í…ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5502/5502 [01:27<00:00, 62.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²´ ë²•ë ¹ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: ì´ 5502ê°œ ë²•ë ¹\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from konlpy.tag import Mecab\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# (ì´í•˜ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤)\n",
    "\n",
    "try:\n",
    "    # â˜… íƒ­ìœ¼ë¡œ ë¶„ë¦¬ëœ ê³¼ëª© ëª©ë¡ íŒŒì¼ì„ ì½ë„ë¡ sep='\\t' ì¶”ê°€\n",
    "    public_laws = pd.read_csv('ë³€í˜¸ì‚¬ì‹œí—˜_ì¶œì œëŒ€ìƒ_ë¶€ì†ë²•ë ¹(ê³µë²•).csv')['ë²•ë ¹ëª…'].tolist()\n",
    "    civil_laws = pd.read_csv('ë³€í˜¸ì‚¬ì‹œí—˜_ì¶œì œëŒ€ìƒ_ë¶€ì†ë²•ë ¹(ë¯¼ì‚¬ë²•).csv')['ë²•ë ¹ëª…'].tolist()\n",
    "    criminal_laws = pd.read_csv('ë³€í˜¸ì‚¬ì‹œí—˜_ì¶œì œëŒ€ìƒ_ë¶€ì†ë²•ë ¹(í˜•ì‚¬ë²•).csv')['ë²•ë ¹ëª…'].tolist()\n",
    "    print(\"ê³¼ëª©ë³„ ë²•ë ¹ ëª©ë¡ ë¡œë“œ ì™„ë£Œ.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ì˜¤ë¥˜: ê³¼ëª©ë³„ ë²•ë ¹ ëª©ë¡ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ({e.filename})\")\n",
    "    exit() # ìŠ¤í¬ë¦½íŠ¸ì—ì„œëŠ” ì£¼ì„ í•´ì œ\n",
    "\n",
    "# --- ì „ì²´ ë²•ë ¹ ë°ì´í„° ë¡œë“œ (ìƒì„±ëœ .txt íŒŒì¼ ë¡œë“œ) ---\n",
    "all_laws_dir = 'ë²•ë ¹_í†µí•©í…ìŠ¤íŠ¸(í˜„ì—­)' # â˜… .txt íŒŒì¼ë“¤ì´ ì €ì¥ëœ í´ë”\n",
    "all_laws_data = []\n",
    "\n",
    "\n",
    "print(f\"'{all_laws_dir}' í´ë”ì—ì„œ ì „ì²´ ë²•ë ¹ í…ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...\")\n",
    "if not os.path.isdir(all_laws_dir):\n",
    "    print(f\"ì˜¤ë¥˜: '{all_laws_dir}' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 1ë‹¨ê³„ ì½”ë“œë¥¼ ë¨¼ì € ì‹¤í–‰í–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "    exit()\n",
    "\n",
    "for file_name in tqdm(os.listdir(all_laws_dir)):\n",
    "    if file_name.endswith('.txt'):\n",
    "        law_name = file_name[:-4]  # '.txt' í™•ì¥ì ì œê±°\n",
    "        file_path = os.path.join(all_laws_dir, file_name)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            all_laws_data.append({'ë²•ë ¹ëª…': law_name, 'ë‚´ìš©': content})\n",
    "        except Exception as e:\n",
    "            print(f\"ì—ëŸ¬: {file_name} íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {e}\")\n",
    "\n",
    "df_all_laws = pd.DataFrame(all_laws_data)\n",
    "print(f\"ì „ì²´ ë²•ë ¹ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: ì´ {len(df_all_laws)}ê°œ ë²•ë ¹\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "276eb56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ (ëª…ì‚¬ ì¶”ì¶œ) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4458/5502 [30:32<07:09,  2.43it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# 'ë‚´ìš©' ì»¬ëŸ¼ì— ì „ì²˜ë¦¬ í•¨ìˆ˜ ì ìš©. ì‹œê°„ì´ ë‹¤ì†Œ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m tqdm\u001b[38;5;241m.\u001b[39mpandas()\n\u001b[1;32m---> 22\u001b[0m df_all_laws[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124më‚´ìš©_ì „ì²˜ë¦¬\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_all_laws\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43më‚´ìš©\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124më°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_all_laws[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124më²•ë ¹ëª…\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124më‚´ìš©_ì „ì²˜ë¦¬\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32mc:\\Users\\Javier\\anaconda3\\envs\\legal_text\\lib\\site-packages\\tqdm\\std.py:917\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[1;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(df, df_function)(wrapper, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Javier\\anaconda3\\envs\\legal_text\\lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Javier\\anaconda3\\envs\\legal_text\\lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Javier\\anaconda3\\envs\\legal_text\\lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\Javier\\anaconda3\\envs\\legal_text\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Javier\\anaconda3\\envs\\legal_text\\lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Javier\\anaconda3\\envs\\legal_text\\lib\\site-packages\\tqdm\\std.py:912\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[0;32m    908\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[0;32m    911\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Mecabì„ ì´ìš©í•´ í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ê³  í•œ ê¸€ì ë‹¨ì–´ëŠ” ì œê±°\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Mecab ì‚¬ìš© ì‹œ: nouns = tokenizer.nouns(text)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Okt ì‚¬ìš© ì‹œ: nouns = tokenizer.nouns(text)\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     nouns \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnouns\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# í•œ ê¸€ì ëª…ì‚¬ ì œê±° í›„ ê³µë°±ìœ¼ë¡œ ì—°ê²°\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([noun \u001b[38;5;28;01mfor\u001b[39;00m noun \u001b[38;5;129;01min\u001b[39;00m nouns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(noun) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Javier\\anaconda3\\envs\\legal_text\\lib\\site-packages\\konlpy\\tag\\_okt.py:83\u001b[0m, in \u001b[0;36mOkt.nouns\u001b[1;34m(self, phrase)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnouns\u001b[39m(\u001b[38;5;28mself\u001b[39m, phrase):\n\u001b[0;32m     81\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Noun extractor.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m     tagged \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [s \u001b[38;5;28;01mfor\u001b[39;00m s, t \u001b[38;5;129;01min\u001b[39;00m tagged \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNoun\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Javier\\anaconda3\\envs\\legal_text\\lib\\site-packages\\konlpy\\tag\\_okt.py:71\u001b[0m, in \u001b[0;36mOkt.pos\u001b[1;34m(self, phrase, norm, stem, join)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"POS tagger.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03mIn contrast to other classes in this subpackage,\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03mthis POS tagger doesn't have a `flatten` option,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m:param join: If True, returns joined sets of morph and tag.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     69\u001b[0m validate_phrase_inputs(phrase)\n\u001b[1;32m---> 71\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjki\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjpype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBoolean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjpype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBoolean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoArray()\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m join:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 3. ë°ì´í„° ì „ì²˜ë¦¬ (í˜•íƒœì†Œ ë¶„ì„ ë° ëª…ì‚¬ ì¶”ì¶œ)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 3. ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ (ëª…ì‚¬ ì¶”ì¶œ) ---\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Mecabì„ ì´ìš©í•´ í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ê³  í•œ ê¸€ì ë‹¨ì–´ëŠ” ì œê±°\"\"\"\n",
    "    try:\n",
    "        # Mecab ì‚¬ìš© ì‹œ: nouns = tokenizer.nouns(text)\n",
    "        # Okt ì‚¬ìš© ì‹œ: nouns = tokenizer.nouns(text)\n",
    "        nouns = tokenizer.nouns(str(text))\n",
    "        # í•œ ê¸€ì ëª…ì‚¬ ì œê±° í›„ ê³µë°±ìœ¼ë¡œ ì—°ê²°\n",
    "        return ' '.join([noun for noun in nouns if len(noun) > 1])\n",
    "    except Exception as e:\n",
    "        # print(f\"ì „ì²˜ë¦¬ ì˜¤ë¥˜ ë°œìƒ: {e}, ì›ë¬¸: {text[:30]}\")\n",
    "        return \"\" # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜\n",
    "\n",
    "# 'ë‚´ìš©' ì»¬ëŸ¼ì— ì „ì²˜ë¦¬ í•¨ìˆ˜ ì ìš©. ì‹œê°„ì´ ë‹¤ì†Œ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "df_all_laws['ë‚´ìš©_ì „ì²˜ë¦¬'] = df_all_laws['ë‚´ìš©'].progress_apply(preprocess_text)\n",
    "print(\"ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ.\")\n",
    "print(df_all_laws[['ë²•ë ¹ëª…', 'ë‚´ìš©_ì „ì²˜ë¦¬']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b9b651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "              ğŸ” ì „ì²˜ë¦¬ ê³¼ì • ì§„ë‹¨ ì‹œì‘\n",
      "==================================================\n",
      "\n",
      "[1ë‹¨ê³„] ì›ë³¸ í…ìŠ¤íŠ¸ ìƒ˜í”Œ (ì• 200ì):\n",
      "ì œ1ì¡°(ëª©ì ) ì´ ê·œì¹™ì€ ã€Œê³µì¦ì¸ë²•ã€ ì œ13ì¡°ì˜2, ì œ15ì¡°ì˜6, ì œ24ì¡°, ì œ75ì¡° ë° ì œ77ì¡°ì˜9ì œ1í•­ì— ë”°ë¥¸ ê³µì¦ì‚¬ë¬´ì†Œì˜ ì‹œì„¤ ê¸°ì¤€, ì„œë¥˜ì˜ ë³´ì¡´ ë°©ë²•, ë³´ì¡´ ì¥ì†Œ, ë³´ì¡´ ê¸°ê°„, íê¸°, ì„œë¥˜ì¸ê³„ì˜ ê¸°ì¤€, ì„œë¥˜ í†µí•©ë³´ê´€ì‹œì„¤ì˜ ê¸°ì¤€ ë° í—ˆê°€ ì ˆì°¨, ê·¸ ë°–ì˜ ì„¤ë¹„ì— ê´€í•˜ì—¬ í•„ìš”í•œ ì‚¬í•­ì„ ê·œì •í•¨ì„ ëª©ì ìœ¼ë¡œ í•œë‹¤.\n",
      "   \n",
      "ì œ2ì¡°(ì •ì˜) ì´ ê·œì¹™ì—ì„œ ì„œë¥˜ë€ ã€Œê³µ\n",
      "\n",
      "[2ë‹¨ê³„] Okt í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
      "\n",
      "[3ë‹¨ê³„] í˜•íƒœì†Œ ë¶„ì„ê¸°ê°€ ì¶”ì¶œí•œ 'ëª…ì‚¬' ëª©ë¡ (ì• 30ê°œ):\n",
      "['ì œ', 'ëª©ì ', 'ì´', 'ê·œì¹™', 'ê³µì¦ì¸', 'ë²•', 'ì œ', 'ì œ', 'ì œ', 'ì œ', 'ë°', 'ì œ', 'ì œ', 'í•­', 'ê³µì¦', 'ì‚¬ë¬´ì†Œ', 'ì‹œì„¤', 'ê¸°ì¤€', 'ì„œë¥˜', 'ë³´ì¡´', 'ë°©ë²•', 'ë³´ì¡´', 'ì¥ì†Œ', 'ë³´ì¡´', 'ê¸°ê°„', 'íê¸°', 'ì„œë¥˜', 'ì¸ê³„', 'ê¸°ì¤€', 'ì„œë¥˜']\n",
      "\n",
      "[4ë‹¨ê³„] í•œ ê¸€ìë¥¼ ì œì™¸í•˜ê³  ë‚¨ì€ 'ëª…ì‚¬' ëª©ë¡ (ì• 30ê°œ):\n",
      "['ëª©ì ', 'ê·œì¹™', 'ê³µì¦ì¸', 'ê³µì¦', 'ì‚¬ë¬´ì†Œ', 'ì‹œì„¤', 'ê¸°ì¤€', 'ì„œë¥˜', 'ë³´ì¡´', 'ë°©ë²•', 'ë³´ì¡´', 'ì¥ì†Œ', 'ë³´ì¡´', 'ê¸°ê°„', 'íê¸°', 'ì„œë¥˜', 'ì¸ê³„', 'ê¸°ì¤€', 'ì„œë¥˜', 'í†µí•©', 'ë³´ê´€', 'ì‹œì„¤', 'ê¸°ì¤€', 'í—ˆê°€', 'ì ˆì°¨', 'ì„¤ë¹„', 'ì‚¬í•­', 'ê·œì •', 'ëª©ì ', 'ì •ì˜']\n",
      "\n",
      "[5ë‹¨ê³„] ìµœì¢… ì „ì²˜ë¦¬ ê²°ê³¼ (ì• 200ì):\n",
      "ëª©ì  ê·œì¹™ ê³µì¦ì¸ ê³µì¦ ì‚¬ë¬´ì†Œ ì‹œì„¤ ê¸°ì¤€ ì„œë¥˜ ë³´ì¡´ ë°©ë²• ë³´ì¡´ ì¥ì†Œ ë³´ì¡´ ê¸°ê°„ íê¸° ì„œë¥˜ ì¸ê³„ ê¸°ì¤€ ì„œë¥˜ í†µí•© ë³´ê´€ ì‹œì„¤ ê¸°ì¤€ í—ˆê°€ ì ˆì°¨ ì„¤ë¹„ ì‚¬í•­ ê·œì • ëª©ì  ì •ì˜ ê·œì¹™ ì„œë¥˜ ê³µì¦ì¸ ì´í•˜ ì„œë¥˜ ë§ˆì´í¬ë¡œí•„ë¦„ ì „ì‚° ì •ë³´ì²˜ë¦¬ ì¡°ì§ ì´í•˜ ì „ì‚° ì •ë³´ì²˜ë¦¬ ì¡°ì§ ë³´ì¡´ ì„œë¥˜ ì„œë¥˜ ë³´ì¡´ ë°©ë²• ì¦ì„œ ì›ë³¸ ì‚¬ì„œ ì¦ì„œ ì¸ì¦ì„œ ì‚¬ë³¸ ì •ê´€ ë²•ì¸ ì˜ì‚¬ë¡ í‘œì§€ ë¶™ì´ ì¦ì„œ ë²ˆí˜¸ ë“±ë¶€ ë²ˆ\n"
     ]
    }
   ],
   "source": [
    "# 2. ë¹„ì–´ìˆì§€ ì•Šì€ ë²•ë ¹ í…ìŠ¤íŠ¸ í•˜ë‚˜ë¥¼ ìƒ˜í”Œë¡œ ì„ íƒ\n",
    "sample_text = \"\"\n",
    "for text in df_all_laws['ë‚´ìš©']:\n",
    "    if text and text.strip():\n",
    "        sample_text = text\n",
    "        break\n",
    "\n",
    "if not sample_text:\n",
    "    print(\"ì˜¤ë¥˜: ë°ì´í„°í”„ë ˆì„ì˜ 'ë‚´ìš©' ì»¬ëŸ¼ì´ ëª¨ë‘ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. íŒŒì¼ ë¡œë”© ê³¼ì •ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "else:\n",
    "    print(\"=\"*50)\n",
    "    print(\"              ğŸ” ì „ì²˜ë¦¬ ê³¼ì • ì§„ë‹¨ ì‹œì‘\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # --- ì§„ë‹¨ ì‹œì‘ ---\n",
    "    print(\"\\n[1ë‹¨ê³„] ì›ë³¸ í…ìŠ¤íŠ¸ ìƒ˜í”Œ (ì• 200ì):\")\n",
    "    print(sample_text[:200])\n",
    "\n",
    "    # í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "    try:\n",
    "        tokenizer = Mecab()\n",
    "        print(\"\\n[2ë‹¨ê³„] Mecab í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "    except:\n",
    "        from konlpy.tag import Okt\n",
    "        tokenizer = Okt()\n",
    "        print(\"\\n[2ë‹¨ê³„] Okt í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    # ëª…ì‚¬ ì¶”ì¶œ ì‹¤í–‰\n",
    "    nouns_list = tokenizer.nouns(sample_text)\n",
    "    print(\"\\n[3ë‹¨ê³„] í˜•íƒœì†Œ ë¶„ì„ê¸°ê°€ ì¶”ì¶œí•œ 'ëª…ì‚¬' ëª©ë¡ (ì• 30ê°œ):\")\n",
    "    print(nouns_list[:30])\n",
    "\n",
    "    # í•œ ê¸€ì ëª…ì‚¬ ì œê±°\n",
    "    filtered_nouns = [noun for noun in nouns_list if len(noun) > 1]\n",
    "    print(\"\\n[4ë‹¨ê³„] í•œ ê¸€ìë¥¼ ì œì™¸í•˜ê³  ë‚¨ì€ 'ëª…ì‚¬' ëª©ë¡ (ì• 30ê°œ):\")\n",
    "    print(filtered_nouns[:30])\n",
    "\n",
    "    # ìµœì¢… ê²°ê³¼\n",
    "    final_result = \" \".join(filtered_nouns)\n",
    "    print(\"\\n[5ë‹¨ê³„] ìµœì¢… ì „ì²˜ë¦¬ ê²°ê³¼ (ì• 200ì):\")\n",
    "    print(final_result[:200])\n",
    "    \n",
    "    if not final_result:\n",
    "        print(\"\\nğŸš¨ ì§„ë‹¨ ê²°ê³¼: ìµœì¢… ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.\")\n",
    "        if not nouns_list:\n",
    "            print(\"-> ì›ì¸: 3ë‹¨ê³„ì—ì„œ í˜•íƒœì†Œ ë¶„ì„ê¸°ê°€ ëª…ì‚¬ë¥¼ ì „í˜€ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. KoNLPyë‚˜ Mecab ì„¤ì¹˜ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "        elif not filtered_nouns:\n",
    "            print(\"-> ì›ì¸: 4ë‹¨ê³„ì—ì„œ ëª¨ë“  ëª…ì‚¬ê°€ í•œ ê¸€ìë¼ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤. ë²•ë ¹ í…ìŠ¤íŠ¸ íŠ¹ì„±ì¼ ìˆ˜ ìˆìœ¼ë‚˜, 3ë‹¨ê³„ ê²°ê³¼ê°€ ì´ìƒí•˜ë‹¤ë©´ í† í¬ë‚˜ì´ì € ë¬¸ì œë¥¼ ì˜ì‹¬í•´ë´ì•¼ í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39e532c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²˜ë¦¬ ê²°ê³¼ê°€ 'preprocessed_laws.csv' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# # --- â˜…â˜…â˜… ì „ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ ì½”ë“œ (ì´ ë¶€ë¶„ì„ ì¶”ê°€í•˜ì„¸ìš”) â˜…â˜…â˜… ---\n",
    "# # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "# df_to_save = df_all_laws[['ë²•ë ¹ëª…', 'ë‚´ìš©', 'ë‚´ìš©_ì „ì²˜ë¦¬']]\n",
    "\n",
    "# # CSV íŒŒì¼ë¡œ ì €ì¥ (index=FalseëŠ” ë¶ˆí•„ìš”í•œ ì¸ë±ìŠ¤ ì»¬ëŸ¼ì´ ìƒê¸°ëŠ” ê²ƒì„ ë°©ì§€)\n",
    "# output_filename = 'preprocessed_laws.csv'\n",
    "# df_to_save.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# print(f\"ì „ì²˜ë¦¬ ê²°ê³¼ê°€ '{output_filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfec8cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ë¥¼ 'ë²•ë ¹_ì „ì²˜ë¦¬_í…ìŠ¤íŠ¸' í´ë”ì— ê°œë³„ ì €ì¥í•©ë‹ˆë‹¤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5502/5502 [00:11<00:00, 466.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ì‘ì—… ì™„ë£Œ! ëª¨ë“  ì „ì²˜ë¦¬ëœ ë²•ë ¹ì´ 'ë²•ë ¹_ì „ì²˜ë¦¬_í…ìŠ¤íŠ¸' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # ì „ì²˜ë¦¬ëœ íŒŒì¼ë“¤ì„ ì €ì¥í•  ìƒˆ í´ë” ì´ë¦„\n",
    "# preprocessed_output_dir = 'ë²•ë ¹_ì „ì²˜ë¦¬_í…ìŠ¤íŠ¸' \n",
    "\n",
    "# # ìƒˆ í´ë” ìƒì„± (ì´ë¯¸ ìˆìœ¼ë©´ ë„˜ì–´ê°)\n",
    "# os.makedirs(preprocessed_output_dir, exist_ok=True)\n",
    "# print(f\"\\nì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ë¥¼ '{preprocessed_output_dir}' í´ë”ì— ê°œë³„ ì €ì¥í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "# # ë°ì´í„°í”„ë ˆì„ì„ í•œ ì¤„ì”© ìˆœíšŒí•˜ë©° íŒŒì¼ë¡œ ì €ì¥\n",
    "# # df_all_lawsëŠ” ì „ì²˜ë¦¬ê°€ ì™„ë£Œëœ ë°ì´í„°í”„ë ˆì„ì…ë‹ˆë‹¤.\n",
    "# for index, row in tqdm(df_all_laws.iterrows(), total=df_all_laws.shape[0]):\n",
    "#     law_name = row['ë²•ë ¹ëª…']\n",
    "#     preprocessed_content = row['ë‚´ìš©_ì „ì²˜ë¦¬']\n",
    "    \n",
    "#     # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "#     output_path = os.path.join(preprocessed_output_dir, f\"{law_name}.txt\")\n",
    "    \n",
    "#     # ì „ì²˜ë¦¬ëœ ë‚´ìš©ì„ .txt íŒŒì¼ë¡œ ì €ì¥\n",
    "#     with open(output_path, 'w', encoding='utf-8') as f:\n",
    "#         f.write(preprocessed_content)\n",
    "\n",
    "# print(f\"\\nì‘ì—… ì™„ë£Œ! ëª¨ë“  ì „ì²˜ë¦¬ëœ ë²•ë ¹ì´ '{preprocessed_output_dir}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c1d9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5502/5502 [31:23<00:00,  2.92it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ.\n",
      "\n",
      "-> ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ë¥¼ 'ë²•ë ¹_ì „ì²˜ë¦¬_í…ìŠ¤íŠ¸' í´ë”ì— ê°œë³„ ì €ì¥í•©ë‹ˆë‹¤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5502/5502 [00:15<00:00, 363.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ì €ì¥ ì„±ê³µ! 'ë²•ë ¹_ì „ì²˜ë¦¬_í…ìŠ¤íŠ¸' í´ë”ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 3. ë°ì´í„° ì „ì²˜ë¦¬ ---\n",
    "print(\"\\n--- ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ ---\")\n",
    "df_all_laws['ë‚´ìš©_ì „ì²˜ë¦¬'] = df_all_laws['ë‚´ìš©'].progress_apply(preprocess_text)\n",
    "print(\"-> ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ.\")\n",
    "\n",
    "# --- 4. ì „ì²˜ë¦¬ ê²°ê³¼ë¥¼ ê°œë³„ txt íŒŒì¼ë¡œ ì €ì¥ ---\n",
    "preprocessed_output_dir = 'ë²•ë ¹_ì „ì²˜ë¦¬_í…ìŠ¤íŠ¸'\n",
    "os.makedirs(preprocessed_output_dir, exist_ok=True)\n",
    "print(f\"\\n-> ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ë¥¼ '{preprocessed_output_dir}' í´ë”ì— ê°œë³„ ì €ì¥í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "try:\n",
    "    for index, row in tqdm(df_all_laws.iterrows(), total=df_all_laws.shape[0]):\n",
    "        law_name = row['ë²•ë ¹ëª…']\n",
    "        preprocessed_content = row['ë‚´ìš©_ì „ì²˜ë¦¬']\n",
    "        output_path = os.path.join(preprocessed_output_dir, f\"{law_name}.txt\")\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(preprocessed_content)\n",
    "    print(f\"\\nâœ… ì €ì¥ ì„±ê³µ! '{preprocessed_output_dir}' í´ë”ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.\")\n",
    "except Exception as e:\n",
    "    print(f\"ğŸš¨ ì €ì¥ ì‹¤íŒ¨: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831ad637",
   "metadata": {},
   "source": [
    "### ìœ ì‚¬ë„ ë¶„ì„í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ab25a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_cluster_similarity(all_vectors, all_law_names, subject_law_list, subject_name):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ ê³¼ëª© í´ëŸ¬ìŠ¤í„°ì˜ ë‚´ë¶€ ìœ ì‚¬ë„ì™€ ì™¸ë¶€ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[{subject_name}] í´ëŸ¬ìŠ¤í„° ìœ ì‚¬ë„ ë¶„ì„ ì¤‘...\")\n",
    "    subject_indices = all_law_names[all_law_names.isin(subject_law_list)].index.tolist()\n",
    "    external_indices = all_law_names[~all_law_names.isin(subject_law_list)].index.tolist()\n",
    "\n",
    "    if len(subject_indices) == 0:\n",
    "        return 0, 0\n",
    "\n",
    "    subject_vectors = all_vectors[subject_indices]\n",
    "    external_vectors = all_vectors[external_indices]\n",
    "    \n",
    "    internal_sim_avg = 0\n",
    "    if subject_vectors.shape[0] > 1:\n",
    "      internal_sim_matrix = cosine_similarity(subject_vectors)\n",
    "      internal_sim_avg = np.mean(internal_sim_matrix[np.triu_indices_from(internal_sim_matrix, k=1)])\n",
    "\n",
    "    external_sim_avg = 0\n",
    "    if len(external_indices) > 0 and len(subject_indices) > 0:\n",
    "      external_sim_matrix = cosine_similarity(subject_vectors, external_vectors)\n",
    "      external_sim_avg = np.mean(external_sim_matrix)\n",
    "\n",
    "    print(f\" - ë‚´ë¶€ ìœ ì‚¬ë„: {internal_sim_avg:.4f}, ì™¸ë¶€ ìœ ì‚¬ë„: {external_sim_avg:.4f}\")\n",
    "    return internal_sim_avg, external_sim_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7777567",
   "metadata": {},
   "source": [
    "# TF-IDF ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1bb00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TF-IDF ë¶„ì„ ì‹œì‘ ---\n",
      "TF-IDF ë²¡í„°í™” ì™„ë£Œ. ë²¡í„° shape: (5502, 5000)\n",
      "\n",
      "[ê³µë²• (TF-IDF)] í´ëŸ¬ìŠ¤í„° ìœ ì‚¬ë„ ë¶„ì„ ì¤‘...\n",
      " - ë‚´ë¶€ ìœ ì‚¬ë„: 0.0946, ì™¸ë¶€ ìœ ì‚¬ë„: 0.0538\n",
      "\n",
      "[ë¯¼ì‚¬ë²• (TF-IDF)] í´ëŸ¬ìŠ¤í„° ìœ ì‚¬ë„ ë¶„ì„ ì¤‘...\n",
      " - ë‚´ë¶€ ìœ ì‚¬ë„: 0.1214, ì™¸ë¶€ ìœ ì‚¬ë„: 0.0445\n",
      "\n",
      "[í˜•ì‚¬ë²• (TF-IDF)] í´ëŸ¬ìŠ¤í„° ìœ ì‚¬ë„ ë¶„ì„ ì¤‘...\n",
      " - ë‚´ë¶€ ìœ ì‚¬ë„: 0.1103, ì™¸ë¶€ ìœ ì‚¬ë„: 0.0520\n",
      "\n",
      "\n",
      "==============================\n",
      "     TF-IDF ë¶„ì„ ê²°ê³¼\n",
      "==============================\n",
      "       ë‚´ë¶€ ìœ ì‚¬ë„    ì™¸ë¶€ ìœ ì‚¬ë„\n",
      "ê³¼ëª©ëª…                    \n",
      "ê³µë²•   0.094576  0.053769\n",
      "ë¯¼ì‚¬ë²•  0.121437  0.044462\n",
      "í˜•ì‚¬ë²•  0.110265  0.052035\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# --- 4-1. TF-IDF ë²¡í„°í™” ---\n",
    "print(\"\\n--- TF-IDF ë¶„ì„ ì‹œì‘ ---\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, min_df=5)\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(df_all_laws['ë‚´ìš©_ì „ì²˜ë¦¬'])\n",
    "print(\"TF-IDF ë²¡í„°í™” ì™„ë£Œ. ë²¡í„° shape:\", tfidf_vectors.shape)\n",
    "\n",
    "# --- 6-1. TF-IDF ìœ ì‚¬ë„ ë¶„ì„ ì‹¤í–‰ ---\n",
    "tfidf_results = []\n",
    "subjects = {\"ê³µë²•\": public_laws, \"ë¯¼ì‚¬ë²•\": civil_laws, \"í˜•ì‚¬ë²•\": criminal_laws}\n",
    "all_law_names_series = df_all_laws['ë²•ë ¹ëª…']\n",
    "\n",
    "for subject_name, law_list in subjects.items():\n",
    "    internal_sim, external_sim = analyze_cluster_similarity(\n",
    "        tfidf_vectors, all_law_names_series, law_list, f\"{subject_name} (TF-IDF)\"\n",
    "    )\n",
    "    tfidf_results.append({\n",
    "        \"ê³¼ëª©ëª…\": subject_name,\n",
    "        \"ë‚´ë¶€ ìœ ì‚¬ë„\": internal_sim,\n",
    "        \"ì™¸ë¶€ ìœ ì‚¬ë„\": external_sim\n",
    "    })\n",
    "\n",
    "# --- 7-1. TF-IDF ìµœì¢… ê²°ê³¼ ì¶œë ¥ ---\n",
    "print(\"\\n\\n\" + \"=\"*30)\n",
    "print(\"     TF-IDF ë¶„ì„ ê²°ê³¼\")\n",
    "print(\"=\"*30)\n",
    "df_tfidf_results = pd.DataFrame(tfidf_results).set_index('ê³¼ëª©ëª…')\n",
    "print(df_tfidf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbceb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf_results.to_csv(\"sbert_ê³¼ëª©ë³„_ê²°ê³¼_í˜„ì—­.idf\", encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9320ce56",
   "metadata": {},
   "source": [
    "### SBERT ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592f25ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ë²•ë ¹_í†µí•©í…ìŠ¤íŠ¸(í˜„ì—­)' í´ë”ì—ì„œ ì›ë³¸ ë²•ë ¹ í…ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5502/5502 [00:00<00:00, 5582.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ì›ë³¸ ë°ì´í„° ë¡œë”© ì™„ë£Œ: ì´ 5502ê°œ ë²•ë ¹\n",
      "ê³¼ëª©ë³„ ë²•ë ¹ ëª©ë¡ ë¡œë“œ ì™„ë£Œ.\n",
      "\n",
      "\n",
      "--- SBERT ë¶„ì„ ì‹œì‘ ---\n",
      "SBERT ë¶„ì„ì— ì‚¬ìš©í•  ì¥ì¹˜: CUDA\n",
      "SBERT ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤. ìµœì´ˆ ì‹¤í–‰ ì‹œ ìˆ˜ ë¶„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\silve\\anaconda3\\envs\\nlp\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\silve\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-multilingual-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT ì¸ì½”ë”©ì„ ì‹œì‘í•©ë‹ˆë‹¤. ë²•ë ¹ ê°œìˆ˜ì— ë”°ë¼ ë§ì€ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 172/172 [00:23<00:00,  7.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT ë²¡í„°í™” ì™„ë£Œ. ë²¡í„° shape: (5502, 768)\n",
      "\n",
      "[ê³µë²• (SBERT)] í´ëŸ¬ìŠ¤í„° ìœ ì‚¬ë„ ë¶„ì„ ì¤‘...\n",
      " - ë‚´ë¶€ ìœ ì‚¬ë„: 0.7114, ì™¸ë¶€ ìœ ì‚¬ë„: 0.6637\n",
      "\n",
      "[ë¯¼ì‚¬ë²• (SBERT)] í´ëŸ¬ìŠ¤í„° ìœ ì‚¬ë„ ë¶„ì„ ì¤‘...\n",
      " - ë‚´ë¶€ ìœ ì‚¬ë„: 0.7786, ì™¸ë¶€ ìœ ì‚¬ë„: 0.6843\n",
      "\n",
      "[í˜•ì‚¬ë²• (SBERT)] í´ëŸ¬ìŠ¤í„° ìœ ì‚¬ë„ ë¶„ì„ ì¤‘...\n",
      " - ë‚´ë¶€ ìœ ì‚¬ë„: 0.6869, ì™¸ë¶€ ìœ ì‚¬ë„: 0.6564\n",
      "\n",
      "\n",
      "==============================\n",
      "      SBERT ë¶„ì„ ê²°ê³¼\n",
      "==============================\n",
      "       ë‚´ë¶€ ìœ ì‚¬ë„    ì™¸ë¶€ ìœ ì‚¬ë„\n",
      "ê³¼ëª©ëª…                    \n",
      "ê³µë²•   0.711350  0.663651\n",
      "ë¯¼ì‚¬ë²•  0.778606  0.684268\n",
      "í˜•ì‚¬ë²•  0.686883  0.656386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# --- 1. ì›ë³¸ í…ìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ---\n",
    "# SBERTëŠ” ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ, ì›ë³¸ .txt íŒŒì¼ì´ ìˆëŠ” í´ë”ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "original_text_dir = 'ë²•ë ¹_í†µí•©í…ìŠ¤íŠ¸(í˜„ì—­)' \n",
    "all_laws_data = []\n",
    "\n",
    "print(f\"'{original_text_dir}' í´ë”ì—ì„œ ì›ë³¸ ë²•ë ¹ í…ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...\")\n",
    "if not os.path.isdir(original_text_dir):\n",
    "    print(f\"ì˜¤ë¥˜: '{original_text_dir}' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›ë³¸ txt í†µí•© íŒŒì¼ì´ ì¤€ë¹„ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "    exit()\n",
    "\n",
    "for filename in tqdm(os.listdir(original_text_dir)):\n",
    "    if filename.endswith('.txt'):\n",
    "        law_name = filename[:-4]\n",
    "        file_path = os.path.join(original_text_dir, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        all_laws_data.append({\n",
    "            'ë²•ë ¹ëª…': law_name,\n",
    "            'ë‚´ìš©': content  # SBERTëŠ” ì „ì²˜ë¦¬ ì „ 'ë‚´ìš©' ì»¬ëŸ¼ì„ ì‚¬ìš©\n",
    "        })\n",
    "\n",
    "df_all_laws = pd.DataFrame(all_laws_data)\n",
    "print(f\"\\nì›ë³¸ ë°ì´í„° ë¡œë”© ì™„ë£Œ: ì´ {len(df_all_laws)}ê°œ ë²•ë ¹\")\n",
    "\n",
    "\n",
    "# --- 2. ê³¼ëª©ë³„ ë²•ë ¹ ëª©ë¡ ë¡œë“œ ---\n",
    "try:\n",
    "    public_laws = pd.read_csv('ë³€í˜¸ì‚¬ì‹œí—˜_ì¶œì œëŒ€ìƒ_ë¶€ì†ë²•ë ¹(ê³µë²•).csv', sep='\\t')['ë²•ë ¹ëª…'].tolist()\n",
    "    civil_laws = pd.read_csv('ë³€í˜¸ì‚¬ì‹œí—˜_ì¶œì œëŒ€ìƒ_ë¶€ì†ë²•ë ¹(ë¯¼ì‚¬ë²•).csv', sep='\\t')['ë²•ë ¹ëª…'].tolist()\n",
    "    criminal_laws = pd.read_csv('ë³€í˜¸ì‚¬ì‹œí—˜_ì¶œì œëŒ€ìƒ_ë¶€ì†ë²•ë ¹(í˜•ì‚¬ë²•).csv', sep='\\t')['ë²•ë ¹ëª…'].tolist()\n",
    "    print(\"ê³¼ëª©ë³„ ë²•ë ¹ ëª©ë¡ ë¡œë“œ ì™„ë£Œ.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ì˜¤ë¥˜: ê³¼ëª©ë³„ ë²•ë ¹ ëª©ë¡ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ({e.filename})\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- 3. ìœ ì‚¬ë„ ë¶„ì„ í•¨ìˆ˜ ì •ì˜ (TF-IDF ë¶„ì„ ë•Œì™€ ë™ì¼) ---\n",
    "def analyze_cluster_similarity(all_vectors, all_law_names, subject_law_list, subject_name):\n",
    "    print(f\"\\n[{subject_name}] í´ëŸ¬ìŠ¤í„° ìœ ì‚¬ë„ ë¶„ì„ ì¤‘...\")\n",
    "    subject_indices = all_law_names[all_law_names.isin(subject_law_list)].index.tolist()\n",
    "    external_indices = all_law_names[~all_law_names.isin(subject_law_list)].index.tolist()\n",
    "\n",
    "    if len(subject_indices) == 0:\n",
    "        return 0, 0\n",
    "\n",
    "    subject_vectors = all_vectors[subject_indices]\n",
    "    external_vectors = all_vectors[external_indices]\n",
    "\n",
    "    internal_sim_avg = 0\n",
    "    if subject_vectors.shape[0] > 1:\n",
    "        internal_sim_matrix = cosine_similarity(subject_vectors)\n",
    "        internal_sim_avg = np.mean(internal_sim_matrix[np.triu_indices_from(internal_sim_matrix, k=1)])\n",
    "\n",
    "    external_sim_avg = 0\n",
    "    if len(external_indices) > 0 and len(subject_indices) > 0:\n",
    "        external_sim_matrix = cosine_similarity(subject_vectors, external_vectors)\n",
    "        external_sim_avg = np.mean(external_sim_matrix)\n",
    "\n",
    "    print(f\" - ë‚´ë¶€ ìœ ì‚¬ë„: {internal_sim_avg:.4f}, ì™¸ë¶€ ìœ ì‚¬ë„: {external_sim_avg:.4f}\")\n",
    "    return internal_sim_avg, external_sim_avg\n",
    "\n",
    "\n",
    "# --- 4. SBERT ë¶„ì„ ì‹¤í–‰ ---\n",
    "print(\"\\n\\n--- SBERT ë¶„ì„ ì‹œì‘ ---\")\n",
    "\n",
    "# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"SBERT ë¶„ì„ì— ì‚¬ìš©í•  ì¥ì¹˜: {device.upper()}\")\n",
    "\n",
    "# SBERT ëª¨ë¸ ë¡œë“œ\n",
    "print(\"SBERT ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤. ìµœì´ˆ ì‹¤í–‰ ì‹œ ìˆ˜ ë¶„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "sbert_model = SentenceTransformer(\n",
    "    'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"SBERT ì¸ì½”ë”©ì„ ì‹œì‘í•©ë‹ˆë‹¤. ë²•ë ¹ ê°œìˆ˜ì— ë”°ë¼ ë§ì€ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤...\")\n",
    "sbert_vectors = sbert_model.encode(\n",
    "    df_all_laws['ë‚´ìš©'].tolist(), \n",
    "    show_progress_bar=True, \n",
    "    batch_size=32 \n",
    ")\n",
    "print(\"SBERT ë²¡í„°í™” ì™„ë£Œ. ë²¡í„° shape:\", sbert_vectors.shape)\n",
    "\n",
    "\n",
    "# --- 5. SBERT ìœ ì‚¬ë„ ê³„ì‚° ë° ê²°ê³¼ ì·¨í•© ---\n",
    "sbert_results = []\n",
    "subjects = {\"ê³µë²•\": public_laws, \"ë¯¼ì‚¬ë²•\": civil_laws, \"í˜•ì‚¬ë²•\": criminal_laws}\n",
    "all_law_names_series = df_all_laws['ë²•ë ¹ëª…']\n",
    "\n",
    "for subject_name, law_list in subjects.items():\n",
    "    internal_sim, external_sim = analyze_cluster_similarity(\n",
    "        sbert_vectors, all_law_names_series, law_list, f\"{subject_name} (SBERT)\"\n",
    "    )\n",
    "    sbert_results.append({\n",
    "        \"ê³¼ëª©ëª…\": subject_name,\n",
    "        \"ë‚´ë¶€ ìœ ì‚¬ë„\": internal_sim,\n",
    "        \"ì™¸ë¶€ ìœ ì‚¬ë„\": external_sim\n",
    "    })\n",
    "\n",
    "# --- 6. ìµœì¢… ê²°ê³¼ ì¶œë ¥ ---\n",
    "print(\"\\n\\n\" + \"=\"*30)\n",
    "print(\"      SBERT ë¶„ì„ ê²°ê³¼\")\n",
    "print(\"=\"*30)\n",
    "df_sbert_results = pd.DataFrame(sbert_results).set_index('ê³¼ëª©ëª…')\n",
    "print(df_sbert_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4808b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbert_results.to_csv(\"sbert_ê³¼ëª©ë³„_ê²°ê³¼_í˜„ì—­.csv\", encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a34db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal_text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
